{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNyrvsDN9wNUVPq6O12XzSx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maxfomichev-tech/news-parser/blob/main/news_parser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
        "# ‚ïë           TELEGRAM NEWS BOT –¥–ª—è Google Colab                         ‚ïë\n",
        "# ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
        "\n",
        "# ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "# ‚îÇ –Ø–ß–ï–ô–ö–ê 1: –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π                                    ‚îÇ\n",
        "# ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "!pip install -q python-telegram-bot==20.7 aiohttp==3.9.1 beautifulsoup4==4.12.2 \\\n",
        "    python-dotenv==1.0.0 lxml==4.9.3 tenacity==8.2.3 html2text==2020.1.16\n",
        "\n",
        "print(\"‚úÖ –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã\")"
      ],
      "metadata": {
        "id": "QAR0ddxQxihU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8766682-6dcd-4b4d-ff2a-65a926a659ec"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# –ß–∏—Ç–∞–µ–º —Å–µ–∫—Ä–µ—Ç—ã\n",
        "os.environ['TELEGRAM_BOT_TOKEN'] = userdata.get('TELEGRAM_BOT_TOKEN')\n",
        "os.environ['GROQ_API_KEY'] = userdata.get('GROQ_API_KEY')  # –ù–æ–≤—ã–π –∫–ª—é—á!\n",
        "\n",
        "print(\"‚úÖ –¢–æ–∫–µ–Ω—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")\n",
        "\n",
        "if not os.environ['TELEGRAM_BOT_TOKEN']:\n",
        "    print(\"‚ùå TELEGRAM_BOT_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω!\")\n",
        "else:\n",
        "    print(\"‚úÖ TELEGRAM_BOT_TOKEN –Ω–∞ –º–µ—Å—Ç–µ\")\n",
        "\n",
        "if not os.environ['GROQ_API_KEY']:\n",
        "    print(\"‚ùå GROQ_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω!\")\n",
        "    print(\"   –ü–æ–ª—É—á–∏—Ç–µ –∫–ª—é—á: https://console.groq.com/keys\")\n",
        "else:\n",
        "    print(\"‚úÖ GROQ_API_KEY –Ω–∞ –º–µ—Å—Ç–µ\")\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# CONFIG –¥–ª—è Groq\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "config_code = '''\n",
        "import os\n",
        "\n",
        "class Config:\n",
        "    TELEGRAM_TOKEN = os.getenv('TELEGRAM_BOT_TOKEN', '')\n",
        "    GROQ_API_KEY = os.getenv('GROQ_API_KEY', '')\n",
        "    RSS_FEEDS_FILE = \"rss_feeds.txt\"\n",
        "    MAX_NEWS_ITEMS = 30\n",
        "    MAX_CHARS_FOR_ANALYSIS = 8000\n",
        "    TG_MAX_MESSAGE_LENGTH = 4096\n",
        "    COMMAND_COOLDOWN_SECONDS = 30\n",
        "\n",
        "config = Config()\n",
        "'''\n",
        "\n",
        "rss_feeds = '''https://lenta.ru/rss/news\n",
        "https://meduza.io/rss/all\n",
        "https://tass.ru/rss/v2.xml'''\n",
        "\n",
        "# utils.py ‚Äî –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π\n",
        "utils_code = '''\n",
        "import re\n",
        "import html\n",
        "import html2text\n",
        "from typing import List\n",
        "\n",
        "html_converter = html2text.HTML2Text()\n",
        "html_converter.ignore_links = False\n",
        "html_converter.ignore_images = True\n",
        "html_converter.body_width = 0\n",
        "\n",
        "def clean_html(raw_html: str) -> str:\n",
        "    if not raw_html:\n",
        "        return \"\"\n",
        "    try:\n",
        "        text = html_converter.handle(raw_html)\n",
        "        text = re.sub(r'\\\\n{3,}', '\\\\n\\\\n', text)\n",
        "        return text.strip()\n",
        "    except Exception:\n",
        "        clean = re.sub(r'<[^>]+>', '', raw_html)\n",
        "        return html.unescape(clean).strip()\n",
        "\n",
        "def split_message(text: str, max_length: int = 4096) -> List[str]:\n",
        "    if len(text) <= max_length:\n",
        "        return [text]\n",
        "    parts = []\n",
        "    current = \"\"\n",
        "    for paragraph in text.split('\\\\n\\\\n'):\n",
        "        if len(current) + len(paragraph) + 2 > max_length:\n",
        "            if current:\n",
        "                parts.append(current.strip())\n",
        "            current = paragraph\n",
        "        else:\n",
        "            current += \"\\\\n\\\\n\" + paragraph if current else paragraph\n",
        "    if current:\n",
        "        parts.append(current.strip())\n",
        "    return parts\n",
        "\n",
        "class RateLimiter:\n",
        "    def __init__(self, cooldown):\n",
        "        self.cooldown = cooldown\n",
        "        self.last_request = {}\n",
        "\n",
        "    def can_proceed(self, user_id: int) -> bool:\n",
        "        from datetime import datetime\n",
        "        now = datetime.now()\n",
        "        if user_id not in self.last_request:\n",
        "            self.last_request[user_id] = now\n",
        "            return True\n",
        "        elapsed = (now - self.last_request[user_id]).total_seconds()\n",
        "        if elapsed >= self.cooldown:\n",
        "            self.last_request[user_id] = now\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def get_remaining_time(self, user_id: int) -> int:\n",
        "        from datetime import datetime\n",
        "        if user_id not in self.last_request:\n",
        "            return 0\n",
        "        elapsed = (datetime.now() - self.last_request[user_id]).total_seconds()\n",
        "        return max(0, int(self.cooldown - elapsed))\n",
        "'''\n",
        "\n",
        "# rss_fetcher.py ‚Äî –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π\n",
        "rss_fetcher_code = '''\n",
        "import asyncio\n",
        "import aiohttp\n",
        "import xml.etree.ElementTree as ET\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime\n",
        "from typing import List, Optional\n",
        "from utils import clean_html\n",
        "from config import config\n",
        "\n",
        "@dataclass\n",
        "class NewsItem:\n",
        "    title: str\n",
        "    summary: str\n",
        "    link: str\n",
        "    source: str\n",
        "    published: Optional[datetime] = None\n",
        "    raw_date: str = \"\"\n",
        "\n",
        "class RSSFetcher:\n",
        "    def __init__(self, timeout: int = 30):\n",
        "        self.timeout = aiohttp.ClientTimeout(total=timeout)\n",
        "        self.session = None\n",
        "\n",
        "    async def __aenter__(self):\n",
        "        self.session = aiohttp.ClientSession(timeout=self.timeout)\n",
        "        return self\n",
        "\n",
        "    async def __aexit__(self, exc_type, exc_val, exc_tb):\n",
        "        if self.session:\n",
        "            await self.session.close()\n",
        "\n",
        "    async def fetch_feed(self, url: str) -> List[NewsItem]:\n",
        "        if not self.session:\n",
        "            raise RuntimeError(\"Use async with\")\n",
        "\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
        "            'Accept': 'application/rss+xml, application/xml, text/xml, */*',\n",
        "            'Accept-Language': 'ru-RU,ru;q=0.9',\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            async with self.session.get(url, headers=headers) as response:\n",
        "                if response.status != 200:\n",
        "                    print(f\"‚ö†Ô∏è {url}: HTTP {response.status}\")\n",
        "                    return []\n",
        "                content = await response.text()\n",
        "                return self._parse_xml(content, url)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error fetching {url}: {e}\")\n",
        "            return []\n",
        "\n",
        "    def _parse_xml(self, content: str, source_url: str) -> List[NewsItem]:\n",
        "        items = []\n",
        "        try:\n",
        "            root = ET.fromstring(content)\n",
        "        except ET.ParseError as e:\n",
        "            print(f\"XML parse error: {e}\")\n",
        "            return []\n",
        "\n",
        "        ns = {'atom': 'http://www.w3.org/2005/Atom'}\n",
        "        channel = root.find('.//channel')\n",
        "        feed = root.find('.//{http://www.w3.org/2005/Atom}feed')\n",
        "\n",
        "        source_name = source_url.split('/')[2] if '://' in source_url else \"Unknown\"\n",
        "\n",
        "        if channel is not None:\n",
        "            title_elem = channel.find('title')\n",
        "            source_name = title_elem.text if title_elem is not None else source_name\n",
        "            entries = channel.findall('item')\n",
        "            date_tag, desc_tag = 'pubDate', 'description'\n",
        "        elif feed is not None:\n",
        "            title_elem = feed.find('atom:title', ns)\n",
        "            source_name = title_elem.text if title_elem is not None else source_name\n",
        "            entries = feed.findall('atom:entry', ns)\n",
        "            date_tag, desc_tag = 'published', 'summary'\n",
        "        else:\n",
        "            entries = root.findall('.//item') or root.findall('.//entry', ns)\n",
        "            date_tag, desc_tag = 'pubDate', 'description'\n",
        "\n",
        "        for entry in entries[:10]:\n",
        "            try:\n",
        "                title = self._get_text(entry, 'title', ns) or \"–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞\"\n",
        "                link = self._get_link(entry, ns)\n",
        "                summary = self._get_text(entry, desc_tag, ns) or \"\"\n",
        "\n",
        "                items.append(NewsItem(\n",
        "                    title=title.strip(),\n",
        "                    summary=clean_html(summary)[:300],\n",
        "                    link=link,\n",
        "                    source=source_name,\n",
        "                    raw_date=self._get_text(entry, date_tag, ns) or \"\"\n",
        "                ))\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "        return items\n",
        "\n",
        "    def _get_text(self, element, tag: str, ns: dict) -> Optional[str]:\n",
        "        if ns:\n",
        "            elem = element.find(f'atom:{tag}', ns)\n",
        "            if elem is not None and elem.text:\n",
        "                return elem.text\n",
        "        elem = element.find(tag)\n",
        "        if elem is not None and elem.text:\n",
        "            return elem.text\n",
        "        return None\n",
        "\n",
        "    def _get_link(self, element, ns: dict) -> str:\n",
        "        link_elem = element.find('atom:link', ns) if ns else None\n",
        "        if link_elem is not None:\n",
        "            href = link_elem.get('href')\n",
        "            if href:\n",
        "                return href\n",
        "\n",
        "        link_elem = element.find('link')\n",
        "        if link_elem is not None and link_elem.text:\n",
        "            return link_elem.text\n",
        "\n",
        "        guid = element.find('guid')\n",
        "        if guid is not None and guid.text:\n",
        "            return guid.text\n",
        "        return \"\"\n",
        "\n",
        "    async def fetch_all(self, urls: List[str]) -> List[NewsItem]:\n",
        "        tasks = [self.fetch_feed(url) for url in urls]\n",
        "        results = await asyncio.gather(*tasks, return_exceptions=True)\n",
        "\n",
        "        all_items = []\n",
        "        for result in results:\n",
        "            if isinstance(result, list):\n",
        "                all_items.extend(result)\n",
        "            elif isinstance(result, Exception):\n",
        "                print(f\"Task failed: {result}\")\n",
        "\n",
        "        return all_items[:config.MAX_NEWS_ITEMS]\n",
        "\n",
        "def load_rss_urls(filename: str) -> List[str]:\n",
        "    try:\n",
        "        with open(filename, 'r', encoding='utf-8') as f:\n",
        "            urls = []\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if not line or line.startswith('#'):\n",
        "                    continue\n",
        "                if line.startswith(('http://', 'https://')):\n",
        "                    urls.append(line)\n",
        "            return urls\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File {filename} not found\")\n",
        "        return []\n",
        "'''\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# AI ANALYZER –¥–ª—è Groq (–±—ã—Å—Ç—Ä—ã–π –∏ –±–µ—Å–ø–ª–∞—Ç–Ω—ã–π)\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "ai_analyzer_code = '''\n",
        "import aiohttp\n",
        "from config import config\n",
        "\n",
        "class AIAnalyzer:\n",
        "    def __init__(self):\n",
        "        self.api_key = config.GROQ_API_KEY\n",
        "        self.url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
        "        self.model = \"llama-3.1-8b-instant\"\n",
        "\n",
        "    async def analyze_news(self, news_text: str) -> str:\n",
        "        headers = {\n",
        "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
        "            \"Content-Type\": \"application/json\",\n",
        "        }\n",
        "\n",
        "        prompt = f\"\"\"–¢—ã ‚Äî –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫ –Ω–æ–≤–æ—Å—Ç–µ–π. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –Ω–æ–≤–æ—Å—Ç–∏ –∏ —Å–æ–∑–¥–∞–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç—á–µ—Ç.\n",
        "\n",
        "üìä –û—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ–º—ã (3-5 —Ç–µ–º —Å –∫—Ä–∞—Ç–∫–∏–º –æ–ø–∏—Å–∞–Ω–∏–µ–º)\n",
        "üî• –ì–ª–∞–≤–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è (2-3 —Å–æ–±—ã—Ç–∏—è —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º)\n",
        "üìà –¢—Ä–µ–Ω–¥—ã (—á—Ç–æ –Ω–∞–±–∏—Ä–∞–µ—Ç –æ–±–æ—Ä–æ—Ç—ã)\n",
        "üí° –í—ã–≤–æ–¥ (–æ–±—â–∞—è –æ—Ü–µ–Ω–∫–∞ —Å–∏—Ç—É–∞—Ü–∏–∏)\n",
        "\n",
        "–ë—É–¥—å –æ–±—ä–µ–∫—Ç–∏–≤–Ω—ã–º –∏ –ª–∞–∫–æ–Ω–∏—á–Ω—ã–º.\n",
        "\n",
        "–ù–æ–≤–æ—Å—Ç–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:\n",
        "{news_text[:4000]}\"\"\"\n",
        "\n",
        "        payload = {\n",
        "            \"model\": self.model,\n",
        "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "            \"temperature\": 0.5,\n",
        "            \"max_tokens\": 1000\n",
        "        }\n",
        "\n",
        "        timeout = aiohttp.ClientTimeout(total=30)  # Groq –æ—á–µ–Ω—å –±—ã—Å—Ç—Ä—ã–π\n",
        "\n",
        "        async with aiohttp.ClientSession(timeout=timeout) as session:\n",
        "            async with session.post(\n",
        "                self.url,\n",
        "                headers=headers,\n",
        "                json=payload\n",
        "            ) as response:\n",
        "\n",
        "                if response.status != 200:\n",
        "                    error_text = await response.text()\n",
        "                    raise Exception(f\"API {response.status}: {error_text[:200]}\")\n",
        "\n",
        "                data = await response.json()\n",
        "                return data['choices'][0]['message']['content'].strip()\n",
        "\n",
        "    async def analyze_with_fallback(self, news_text: str) -> str:\n",
        "        try:\n",
        "            return await self.analyze_news(news_text)\n",
        "        except Exception as e:\n",
        "            print(f\"AI Error: {e}\")\n",
        "            lines = [l.strip() for l in news_text.split('\\\\n') if l.strip() and l[0].isdigit()]\n",
        "            headers = [l.split('. ', 1)[1] if '. ' in l else l for l in lines[:5]]\n",
        "\n",
        "            return (\n",
        "                \"‚ö†Ô∏è *–ê–Ω–∞–ª–∏–∑ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω*\\\\n\\\\n\"\n",
        "                f\"_–û—à–∏–±–∫–∞: {str(e)[:100]}_\\\\n\\\\n\"\n",
        "                \"*–ü–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏:*\\\\n\" +\n",
        "                '\\\\n'.join([f\"‚Ä¢ {h[:80]}...\" for h in headers])\n",
        "            )\n",
        "'''\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª—ã\n",
        "files = {\n",
        "    'config.py': config_code,\n",
        "    'rss_feeds.txt': rss_feeds,\n",
        "    'utils.py': utils_code,\n",
        "    'rss_fetcher.py': rss_fetcher_code,\n",
        "    'ai_analyzer.py': ai_analyzer_code\n",
        "}\n",
        "\n",
        "for filename, content in files.items():\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        f.write(content)\n",
        "    print(f\"‚úÖ {filename}\")\n",
        "\n",
        "print(\"\\nüéâ –ì–æ—Ç–æ–≤–æ! –ó–∞–ø—É—Å–∫–∞–π—Ç–µ —è—á–µ–π–∫—É 3\")"
      ],
      "metadata": {
        "id": "KXWzDJSk05ek",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44c64acf-7f41-463b-d5a3-386a6ba338a4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ –¢–æ–∫–µ–Ω—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã\n",
            "‚úÖ TELEGRAM_BOT_TOKEN –Ω–∞ –º–µ—Å—Ç–µ\n",
            "‚úÖ GROQ_API_KEY –Ω–∞ –º–µ—Å—Ç–µ\n",
            "‚úÖ config.py\n",
            "‚úÖ rss_feeds.txt\n",
            "‚úÖ utils.py\n",
            "‚úÖ rss_fetcher.py\n",
            "‚úÖ ai_analyzer.py\n",
            "\n",
            "üéâ –ì–æ—Ç–æ–≤–æ! –ó–∞–ø—É—Å–∫–∞–π—Ç–µ —è—á–µ–π–∫—É 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "from telegram import Update\n",
        "from telegram.ext import Application, CommandHandler, ContextTypes\n",
        "import asyncio\n",
        "\n",
        "# Import modules as objects to allow reloading\n",
        "import config as config_module\n",
        "import rss_fetcher as rss_fetcher_module\n",
        "import ai_analyzer as ai_analyzer_module\n",
        "import utils as utils_module\n",
        "\n",
        "# Reload modules to ensure latest changes are picked up\n",
        "import importlib\n",
        "importlib.reload(config_module)\n",
        "importlib.reload(rss_fetcher_module)\n",
        "importlib.reload(ai_analyzer_module)\n",
        "importlib.reload(utils_module)\n",
        "\n",
        "# Now, use the reloaded objects/classes\n",
        "config = config_module.config\n",
        "RSSFetcher = rss_fetcher_module.RSSFetcher\n",
        "load_rss_urls = rss_fetcher_module.load_rss_urls\n",
        "AIAnalyzer = ai_analyzer_module.AIAnalyzer\n",
        "split_message = utils_module.split_message\n",
        "RateLimiter = utils_module.RateLimiter\n",
        "\n",
        "rate_limiter = RateLimiter(config.COMMAND_COOLDOWN_SECONDS)\n",
        "\n",
        "def prepare_news_text(news_items):\n",
        "    parts = []\n",
        "    total = 0\n",
        "    for i, item in enumerate(news_items, 1):\n",
        "        text = f\"{i}. {item.title}\\n\"\n",
        "        if item.summary:\n",
        "            text += f\"   {item.summary[:150]}\\n\"\n",
        "        text += f\"   [{item.source}]\\n\\n\"\n",
        "        if total + len(text) > config.MAX_CHARS_FOR_ANALYSIS:\n",
        "            break\n",
        "        parts.append(text)\n",
        "        total += len(text)\n",
        "    return \"\".join(parts)\n",
        "\n",
        "async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):\n",
        "    await update.message.reply_text(\n",
        "        \"üëã –ë–æ—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–æ–≤–æ—Å—Ç–µ–π!\\n\\n\"\n",
        "        \"/news ‚Äî –ø–æ–ª—É—á–∏—Ç—å –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—É—â–∏—Ö —Å–æ–±—ã—Ç–∏–π\\n\"\n",
        "        \"/status ‚Äî –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫–∏\\n\"\n",
        "        \"‚è±Ô∏è –ö—É–ª–¥–∞—É–Ω –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏: 30 —Å–µ–∫\"\n",
        "    )\n",
        "\n",
        "async def status_command(update: Update, context: ContextTypes.DEFAULT_TYPE):\n",
        "    urls = load_rss_urls(config.RSS_FEEDS_FILE)\n",
        "    await update.message.reply_text(f\"üìã –ü–æ–¥–∫–ª—é—á–µ–Ω–æ {len(urls)} RSS-–∫–∞–Ω–∞–ª–æ–≤\")\n",
        "\n",
        "async def news_command(update: Update, context: ContextTypes.DEFAULT_TYPE):\n",
        "    user_id = update.effective_user.id\n",
        "\n",
        "    if not rate_limiter.can_proceed(user_id):\n",
        "        remaining = rate_limiter.get_remaining_time(user_id)\n",
        "        await update.message.reply_text(f\"‚è≥ –ü–æ–¥–æ–∂–¥–∏—Ç–µ {remaining} —Å–µ–∫—É–Ω–¥\")\n",
        "        return\n",
        "\n",
        "    msg = await update.message.reply_text(\"‚è≥ –°–æ–±–∏—Ä–∞—é –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ RSS...\")\n",
        "\n",
        "    try:\n",
        "        urls = load_rss_urls(config.RSS_FEEDS_FILE)\n",
        "        if not urls:\n",
        "            await msg.edit_text(\"‚ùå –ù–µ—Ç RSS-–∫–∞–Ω–∞–ª–æ–≤ –≤ rss_feeds.txt\")\n",
        "            return\n",
        "\n",
        "        async with RSSFetcher() as fetcher:\n",
        "            news = await fetcher.fetch_all(urls)\n",
        "\n",
        "        if not news:\n",
        "            await msg.edit_text(\"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –Ω–æ–≤–æ—Å—Ç–∏\")\n",
        "            return\n",
        "\n",
        "        await msg.edit_text(f\"üì• –ü–æ–ª—É—á–µ–Ω–æ {len(news)} –Ω–æ–≤–æ—Å—Ç–µ–π. –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é...\")\n",
        "\n",
        "        news_text = prepare_news_text(news)\n",
        "        analyzer = AIAnalyzer()\n",
        "        analysis = await analyzer.analyze_with_fallback(news_text)\n",
        "\n",
        "        parts = split_message(f\"üìä *–ê–Ω–∞–ª–∏–∑ –Ω–æ–≤–æ—Å—Ç–µ–π*\\n\\n{analysis}\", 4000)\n",
        "\n",
        "        for i, part in enumerate(parts):\n",
        "            if i == 0:\n",
        "                await msg.edit_text(part, parse_mode='Markdown')\n",
        "            else:\n",
        "                await update.message.reply_text(part, parse_mode='Markdown')\n",
        "                await asyncio.sleep(0.5)\n",
        "\n",
        "    except Exception as e:\n",
        "        await msg.edit_text(f\"‚ùå –û—à–∏–±–∫–∞: {str(e)[:200]}\")\n",
        "\n",
        "application = Application.builder().token(config.TELEGRAM_TOKEN).build()\n",
        "application.add_handler(CommandHandler(\"start\", start))\n",
        "application.add_handler(CommandHandler(\"status\", status_command))\n",
        "application.add_handler(CommandHandler(\"news\", news_command))\n",
        "\n",
        "print(\"üöÄ –ë–æ—Ç –∑–∞–ø—É—â–µ–Ω! –û—Ç–ø—Ä–∞–≤—å—Ç–µ /start –≤ Telegram\")\n",
        "print(\"‚ö†Ô∏è –ù–µ –∑–∞–∫—Ä—ã–≤–∞–π—Ç–µ —ç—Ç—É —è—á–µ–π–∫—É\")\n",
        "\n",
        "await application.run_polling()"
      ],
      "metadata": {
        "id": "9-WVb7Cc0623"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}